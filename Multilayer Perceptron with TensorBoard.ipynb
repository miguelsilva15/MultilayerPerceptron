{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing TensorFlow and TensorBoard with multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this for?\n",
    "This is an example of a simple multilayer perceptron coded in tensorflow\n",
    "\n",
    "### What data am I using?\n",
    "\n",
    "I am using the mnist dataset.<br>\n",
    "This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit.In order to utilize <br> an 8x8 figure like this, weâ€™d have to first transform it into a feature vector with length 64.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"http://theanets.readthedocs.io/en/stable/_images/mnist-digits-small.png\" alt=\"Multilayer Perceptron\"/>\n",
    "<br>\n",
    "I am loading this dataset from __[sklearn.datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets)__. <br>\n",
    "This dataset contains 10 classes and approximately 180 samples per class\n",
    "\n",
    "### What architecture will I use?\n",
    "\n",
    "I am going to use a multilayer perceptron with two hidden layer, to mantain non-linearity and to explore the benefits of differents hyperparameters\n",
    "\n",
    "<img src=\"https://elogeel.files.wordpress.com/2010/05/050510_1627_multilayerp1.png\" alt=\"Multilayer Perceptron\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The dataset label have only one dimension but ten values on it.\n",
    "\n",
    "<br> We have to only preserve one class per column to make this architecture work, because we need <br>one neuron to output the given probability of a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "X_data = digits.data\n",
    "y_data = digits.target\n",
    "print(y_data.shape)\n",
    "print(np.unique(y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that matter I am going to use pandas get_dummies function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 10)\n"
     ]
    }
   ],
   "source": [
    "y_data = pd.get_dummies(y_data).as_matrix()\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data in separate train and test slices because we have to test the real precision of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, stratify= y_data,  test_size=0.28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we are going to set the paramaters for our neural network and tensorboard, that is why we are going to log our results, in this case we are going to explore differents learning rates and optimizers to find what is best for our perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "n_inputs = X_data.shape[1]\n",
    "n_outputs = y_data.shape[1]\n",
    "n_hidden_1 = 90\n",
    "n_hidden_2 = 30\n",
    "\n",
    "#log path for every run\n",
    "\n",
    "logs_path_1_run = \"/tmp/mnist/1\"\n",
    "logs_path_2_run = \"/tmp/mnist/2\"\n",
    "\n",
    "\n",
    "# command to open tensorboard\n",
    "\n",
    "# tensorboard --logdir=run1:/tmp/mnist/1 --port=6006\n",
    "# for multiples inputs tensorboard --logdir=run1:/tmp/mnist/1,run2:/tmp/mnist/2 --port=6006\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our architecture based, initializing our neural network with random uniform distribution in our weigths matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "weights_1 = tf.Variable(tf.random_uniform([n_inputs, n_hidden_1], -1.0, 1.0))\n",
    "# hidden layer\n",
    "weights_2 = tf.Variable(tf.random_uniform([n_hidden_1, n_hidden_2], -1.0, 1.0))\n",
    "#output layer\n",
    "weights_3 = tf.Variable(tf.random_uniform([n_hidden_2, n_outputs], -1.0, 1.0))\n",
    "\n",
    "#bias input layer\n",
    "bias_1 = tf.Variable(tf.zeros([n_hidden_1]), name=\"bias2\")\n",
    "\n",
    "#bias for hidden\n",
    "bias_2 = tf.Variable(tf.zeros([n_hidden_2]), name=\"bias1\")\n",
    "\n",
    "#bias output layer\n",
    "bias_3 = tf.Variable(tf.zeros([n_outputs]), name=\"output\")\n",
    "\n",
    "#input layer sigmoid\n",
    "il = tf.sigmoid(tf.matmul(x,weights_1) + bias_1)\n",
    "hl = tf.sigmoid(tf.matmul(il, weights_2) + bias_2)\n",
    "ol = tf.sigmoid(tf.matmul(hl, weights_3) + bias_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try differents optimizers and learning rates here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-1ceec424a67f>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = ol))\n",
    "adam = tf.train.AdamOptimizer(learning_rate=.5e-3).minimize(loss)\n",
    "\n",
    "gdo = tf.train.GradientDescentOptimizer(learning_rate=.7e-1).minimize(loss)\n",
    "\n",
    "#predict the batch \n",
    "correct_prediction = tf.equal(tf.argmax(ol,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# log the loss\n",
    "tf.summary.scalar(\"cost\", loss)\n",
    "\n",
    "# log the accuracy\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# merge all the logs\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\n",
    "        'logs': logs_path_1_run,\n",
    "         'opt': adam\n",
    "    },\n",
    "    {\n",
    "        'logs': logs_path_2_run, \n",
    "         'opt': gdo\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, Y, batch_size, idx):\n",
    "    return X[batch_size*idx:batch_size*(idx+1)], Y[batch_size*idx:batch_size*(idx+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size= 599\n",
    "total_numbers_of_batches = int(X_data.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9563492\n",
      "Accuracy: 0.9047619\n"
     ]
    }
   ],
   "source": [
    "for prmtr in range(len(parameters)):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        writer = tf.summary.FileWriter(parameters[prmtr][\"logs\"], graph=tf.get_default_graph())\n",
    "        for e in range(epochs):\n",
    "            for i in range(total_numbers_of_batches):\n",
    "                batch_x, batch_y = get_batch(X_train, y_train, batch_size, i)\n",
    "                _, c, summary = sess.run([parameters[prmtr][\"opt\"], loss, summary_op], feed_dict = {x: batch_x, y: batch_y})\n",
    "                writer.add_summary(summary, e * batch_size + i)\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(\"Accuracy:\", accuracy.eval({x: X_test, y: y_test}))\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results shown by Tensorboard\n",
    "<br>\n",
    "The Accuracy shown by Adam trained a lot faster than sgd (our second run) and converge more smoothly\n",
    "\n",
    "\n",
    "<img src=\"img_1.png\" alt=\"acc\" style=\"width: 800px;\"/>\n",
    "\n",
    "<br> \n",
    "\n",
    "And our loss curve in adam descends in a good way here too without stumbling too much based on the momentum (read the link below)\n",
    "\n",
    "<img src=\"img_2.png\" alt=\"acc\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "> So our result tested than no optimizer overfitted and adam converged way more faster than sgd, this is based on how adam works. If you have time you can checkout the __[original paper](https://arxiv.org/abs/1412.6980)__. <br>\n",
    "\n",
    "> TensorBoard It gives many advantages like watching the loss function change in training time, also give the posibility of check the architecture (this is our multilayer perceptron with adam).\n",
    "\n",
    "<img src=\"img_3.png\" alt=\"acc\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "> TensorBoard also give us the posibility to dive into more detail explanation of neural networks behavior which generally serve to tweak hyperparameters more consciously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
